{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8463ae6f-c376-4d91-af7d-6a82ef75e684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 1\n",
    "# ans -Example:\n",
    "# Consider flipping a fair coin. The random variable X represents the outcome of a single coin flip. If we get heads, we'll denote it as X = 1, and if we get tails, we'll denote it as X = 0.\n",
    "\n",
    "# 1.> Probability Mass Function (PMF):\n",
    "#    The PMF gives the probability of a discrete random variable taking on specific values. For the fair coin, the PMF can be expressed as follows:\n",
    "#    PMF(X = 1) = P(Heads) = 0.5 (since the coin is fair)\n",
    "#    PMF(X = 0) = P(Tails) = 0.5 (since the coin is fair)\n",
    "\n",
    "#    The PMF tells us the probability of getting heads (X = 1) is 0.5, and the probability of getting tails (X = 0) is also 0.5.\n",
    "\n",
    "\n",
    "# 2.> Probability Density Function (PDF):\n",
    "#    The PDF gives the probability density of a continuous random variable within a certain range. Since the coin flip is a discrete event, we won't use a PDF in this case. Instead, we'll stick to the PMF.\n",
    "#     In summary, the PMF is used for discrete random variables and provides probabilities for specific outcomes, while the PDF is used for continuous random variables and gives probabilities for intervals of values. In this example, flipping a coin involves a discrete random variable, so we only use the Probability Mass Function (PMF) to describe the probabilities of getting heads or tails.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "685b17ae-60cf-42ac-9c91-e7efc6c46a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 2\n",
    "# ans - The Cumulative Density Function (CDF) is a concept used in probability and statistics to describe the probability that a random variable takes on a value less than or equal to a specified value. In other words, it gives the cumulative probability of the random variable being less than or equal to a particular point.\n",
    "\n",
    "# For a discrete random variable X, the CDF is defined as follows:\n",
    "\n",
    "# CDF(x) = P(X ≤ x)\n",
    "\n",
    "# For a continuous random variable X, the CDF is defined as the integral of the Probability Density Function (PDF) from negative infinity up to the specified value:\n",
    "\n",
    "# CDF(x) = ∫[−∞ to x] PDF(t) dt\n",
    "\n",
    "#The CDF provides valuable information about the distribution of the random variable, including the likelihood of certain outcomes occurring up to a given point.\n",
    "\n",
    "# example of cdf ---\n",
    "\n",
    "# Certainly! Let's consider an example of a continuous random variable X that follows an exponential distribution. The exponential distribution is commonly used to model the time between events in a Poisson process, such as the time between arrivals of customers at a service center or the time between occurrences of earthquakes.\n",
    "\n",
    "# The probability density function (PDF) of the exponential distribution is given by:\n",
    "\n",
    "# PDF(x | λ) = λ * exp(-λx) for x ≥ 0, and 0 for x < 0\n",
    "\n",
    "# where λ is the rate parameter, which controls the average rate of occurrence of events. The larger the value of λ, the more frequent the events.\n",
    "\n",
    "# To find the cumulative density function (CDF) of the exponential distribution, we integrate the PDF from negative infinity up to the specified value x:\n",
    "\n",
    "# CDF(x | λ) = ∫[0 to x] λ * exp(-λt) dt\n",
    "\n",
    "#Now, let's calculate the CDF for the exponential distribution with a rate parameter λ = 0.5:\n",
    "\n",
    "# CDF(x | 0.5) = ∫[0 to x] 0.5 * exp(-0.5t) dt\n",
    "\n",
    "# We can then evaluate the integral:\n",
    "\n",
    "# CDF(x | 0.5) = -exp(-0.5t) | [0 to x]\n",
    "\n",
    "# After substituting the limits of integration:\n",
    "\n",
    "#CDF(x | 0.5) = -exp(-0.5x) + exp(0)\n",
    "\n",
    "# Since exp(0) = 1, the CDF becomes:\n",
    "\n",
    "# CDF(x | 0.5) = 1 - exp(-0.5x)\n",
    "\n",
    "# used of cdf is--It is used to describe the probability distribution of random variables in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f66816d7-810b-445a-81a1-903ecab9a6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ques 3-\n",
    "#ans-- The normal distribution, also known as the Gaussian distribution, is a continuous probability distribution that is widely used in statistics, mathematics, and various fields to model and describe a wide range of natural phenomena. It is characterized by its bell-shaped curve, which is symmetrical and centered around its mean (μ). Here are some examples of situations where the normal distribution might be used as a model:\n",
    "\n",
    "#    2.>  Heights of Individuals: In a population, the distribution of heights of individuals is often approximately normal. The mean and standard deviation of the normal distribution can help describe the average height and the variability around that average.\n",
    "\n",
    "#    2.>  Test Scores: When large numbers of people take a standardized test, such as the SAT or IQ test, the distribution of scores often follows a normal distribution. The mean and standard deviation can be used to interpret the average performance and the spread of scores.\n",
    "\n",
    "#  The parameters of the normal distribution, μ (mean) and σ (standard deviation), play essential roles in shaping the distribution:\n",
    "\n",
    "#  Mean (μ): The mean is the central value of the distribution, and it determines where the peak of the bell-shaped curve is located. Shifting the mean to the left or right will move the entire distribution accordingly.\n",
    "\n",
    "#  Standard Deviation (σ): The standard deviation is a measure of the spread or dispersion of the data points in the distribution. A larger standard deviation leads to a broader and flatter curve, while a smaller standard deviation results in a narrower and taller curve.\n",
    "\n",
    "#  Together, the mean and standard deviation dictate the exact shape of the normal distribution, with the bell-shaped curve being symmetrically centered around the mean. The empirical rule, also known as the 68-95-99.7 rule, states that approximately 68%, 95%, and 99.7% of the data falls within one, two, and three standard deviations from the mean, respectively, in a normally distributed data set.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "742aa2af-83f3-4447-8ec4-6d0886373cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 4-\n",
    "#ans - The normal distribution is of paramount importance in various fields due to its mathematical properties and widespread applicability. Some of the key reasons for its importance are:\n",
    "\n",
    "#   1.>  Central Limit Theorem: One of the fundamental aspects of the normal distribution is the Central Limit Theorem. It states that the sum or average of a large number of independent and identically distributed random variables will be approximately normally distributed, regardless of the original distribution of the variables. This property allows us to apply the normal distribution to a wide range of situations where sample sizes are large, making it a fundamental tool in inferential statistics.\n",
    "\n",
    "#   2.>  Predictive Modeling: In many real-world situations, data is influenced by multiple factors and can be highly complex. However, in some cases, the combined effect of these factors leads to a distribution that approximates a normal distribution. This simplifies modeling and makes it easier to make predictions and estimates based on the normal distribution's properties.\n",
    "\n",
    "#   3.>  Statistical Inference: The normal distribution provides a solid foundation for various statistical methods, such as hypothesis testing, confidence intervals, and regression analysis. These techniques are widely used in research, social sciences, economics, and many other disciplines to draw conclusions about populations based on samples.\n",
    "\n",
    "\n",
    "# Real Life example of Normal Distribution are--\n",
    "\n",
    "# 1.>  Human Height: The distribution of heights in a large population typically follows a normal distribution. Most people cluster around the average height, and the number of individuals decreases as you move away from the mean height in either direction.\n",
    "\n",
    "# 2.>  Exam Scores: When a large number of students take an exam, the scores often approximate a normal distribution. The majority of students tend to score around the average, and fewer students score at the extremes (very high or very low).\n",
    "\n",
    "# 3.>  Errors in Measurements: In scientific experiments, measurements are prone to random errors. These errors often follow a normal distribution, where the mean represents the true value, and the standard deviation reflects the variability in the measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43f99e85-cea3-4462-9afd-741a5ea855cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 5\n",
    "#ans-The Bernoulli distribution is a discrete probability distribution that represents a random variable that can take only two possible outcomes: success (usually denoted by 1) with probability p and failure (usually denoted by 0) with probability q = 1 - p. In other words, it models a single binary event where success occurs with a probability of p and failure occurs with a probability of q.\n",
    "\n",
    "# example of bernoulli distribution - Flipping a Coin\n",
    "\n",
    "#Consider flipping a fair (unbiased) coin, where the probability of getting a head (success) is p = 0.5, and the probability of getting a tail (failure) is q = 1 - p = 0.5.\n",
    "\n",
    "#Let X be a random variable representing the outcome of a single coin flip. In this case, X follows a Bernoulli distribution with p = 0.5.\n",
    "\n",
    "# The probability of getting a head (X = 1) is given by:\n",
    "# P(X = 1) = p = 0.5\n",
    "\n",
    "# The probability of getting a tail (X = 0) is given by:\n",
    "#  P(X = 0) = q = 0.5\n",
    "\n",
    "# So, if we flip the coin multiple times and record the outcomes, we expect that, on average, we will get heads approximately half of the time (50% of the flips) and tails the other half of the time (also 50% of the flips). Each individual coin flip is a Bernoulli trial, and the sequence of flips can be modeled using a sequence of independent Bernoulli random variables.\n",
    "\n",
    "# The Bernoulli distribution is a fundamental building block for other important distributions, such as the binomial distribution (a sum of independent Bernoulli random variables) and the geometric distribution (number of trials required to achieve the first success in a sequence of Bernoulli trials).\n",
    "\n",
    "\n",
    "# Difference b/w BErnoulli and Binomial distribution--\n",
    "# The Bernoulli distribution represents the success or failure of a single Bernoulli trial. The Binomial Distribution represents the number of successes and failures in n independent Bernoulli trials for some given value of n.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "852dfdd5-e41b-4156-808d-699a7f033dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 6\n",
    "# To find the probability that a randomly selected observation from a normally distributed dataset will be greater than 60, we need to use the standard normal distribution (z-distribution) and apply the z-score formula. The z-score is a measure of how many standard deviations a data point is from the mean.\n",
    "\n",
    "# The formula to calculate the z-score for a value x in a normally distributed dataset with mean μ and standard deviation σ is:\n",
    "\n",
    "# z = (x - μ) / σ\n",
    "\n",
    "# Let's calculate the z-score for x = 60:\n",
    "\n",
    "#  z = (60 - 50) / 10\n",
    "#  z = 10 / 10\n",
    "#  z = 1\n",
    "\n",
    "#  Now, we need to find the probability that a randomly selected observation from the dataset will be greater than 60, which is equivalent to finding the probability of having a z-score greater than 1 in the standard normal distribution.\n",
    "\n",
    "#   Using a standard normal distribution table or a calculator, we can find the probability corresponding to a z-score of 1. The probability of having a z-score greater than 1 is approximately 0.1587.\n",
    "\n",
    "#  Therefore, the probability that a randomly selected observation from the given normally distributed dataset will be greater than 60 is approximately 0.1587 or 15.87%.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "659c1230-166a-4fef-967b-a4e5a1fa2b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 7 \n",
    "# ans -- The uniform distribution is a probability distribution where all possible outcomes within a specific range are equally likely. In other words, it assigns equal probability to every value in the range, resulting in a flat, constant probability density function.\n",
    "\n",
    "#  The probability density function (PDF) of a continuous uniform distribution over the interval [a, b] is given by:\n",
    "\n",
    "#  f(x) = 1 / (b - a) for a ≤ x ≤ b\n",
    "#  f(x) = 0 elsewhere\n",
    "\n",
    "#  Here, 'a' and 'b' are the minimum and maximum values of the interval, respectively.\n",
    "\n",
    "# The uniform distribution is often used in situations where there is no specific reason to expect any particular outcome to be more likely than others. It is commonly employed in random number generation, simulations, and certain statistical analyses.\n",
    "\n",
    "#  Example: Rolling a Fair Die\n",
    "\n",
    "#  Consider the classic example of rolling a fair six-sided die. The possible outcomes are the integers from 1 to 6. Assuming the die is unbiased and each face is equally likely to come up, we have a uniform distribution.\n",
    "\n",
    "#  Let X be a random variable representing the outcome of rolling the die. The probability of getting each value is:\n",
    "\n",
    "#  P(X = 1) = P(X = 2) = P(X = 3) = P(X = 4) = P(X = 5) = P(X = 6) = 1/6\n",
    "\n",
    "#   Since the die is fair, each outcome has an equal probability of 1/6.\n",
    "\n",
    "#  The probability density function of the uniform distribution for this example is:\n",
    "\n",
    "#  f(x) = 1 / (6 - 1) = 1/5 for 1 ≤ x ≤ 6\n",
    "#  f(x) = 0 elsewhere\n",
    "\n",
    "#  This means that the probability of rolling any specific number between 1 and 6 is 1/6, and the probability of rolling a number outside this range is 0.\n",
    "\n",
    "#   In this case, the uniform distribution is a suitable model for the die because all outcomes have the same chance of occurring, making it a fair game of chance.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a013ff63-ac9a-4bb0-82d7-f9df2dadf4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 8 \n",
    "#ans- The z-score, also known as the standard score or standardized value, is a statistical measure that indicates how many standard deviations a data point is away from the mean of a dataset. It is a dimensionless quantity, which means it has no units and is applicable to any dataset, regardless of its original scale or units of measurement.\n",
    "\n",
    " #  The formula to calculate the z-score for a data point x in a dataset with mean μ and standard deviation σ is:\n",
    "\n",
    "#   z = (x - μ) / σ\n",
    "\n",
    "# The z-score represents the position of a data point relative to the mean in terms of standard deviations. A positive z-score indicates that the data point is above the mean, while a negative z-score indicates that it is below the mean. A z-score of 0 indicates that the data point is exactly at the mean\n",
    "\n",
    "#  Importance of Z-Score:\n",
    "\n",
    "# 1.>  Standardization: The z-score standardizes data, allowing us to compare values from different datasets on the same scale. It removes the effect of the original units and allows us to analyze how extreme or unusual a particular data point is in relation to the rest of the data.\n",
    "\n",
    "#  2.> Outlier Detection: Z-scores are commonly used to identify outliers in a dataset. Data points with z-scores far from zero are considered outliers as they are far from the mean.\n",
    "\n",
    "#  3.> Probability Calculations: Z-scores are used in normal distribution tables to calculate probabilities for specific values. The z-score helps determine the probability of a value occurring in a normally distributed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdf6b82c-7d1a-4e84-8859-a43c303ef905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 9 \n",
    "# ans- The Central Limit Theorem (CLT) is a fundamental concept in statistics that describes the behavior of the sample means (or sums) of a large number of independent and identically distributed random variables. It states that, regardless of the shape of the original population distribution, the distribution of sample means will tend to follow a normal distribution as the sample size increases.\n",
    "\n",
    "# Significance of Central Limit therom are --\n",
    "\n",
    "# 1.> Normal Approximation: The CLT is significant because it allows us to approximate the distribution of sample means with a normal distribution, regardless of the original population's distribution. This simplifies statistical analysis, as many statistical methods and tests assume or work best with normally distributed data.\n",
    "\n",
    "# 2.> Inferential Statistics: The Central Limit Theorem forms the basis for many inferential statistical techniques, such as hypothesis testing and confidence interval estimation. These techniques rely on the normality of the sample means to draw conclusions about population parameters.\n",
    "\n",
    "# 3.> Real-world Applications: The CLT is widely applicable in various fields, including social sciences, natural sciences, finance, engineering, and more. It is particularly valuable when dealing with large datasets or populations where the underlying distribution may not be known or may not be normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9471263b-15c5-4de7-ab49-4de6a553c2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 10 \n",
    "# ans - The assumptions of the Central Limit Theorem are as follows:\n",
    "\n",
    "# 1.>  Random Sampling: The samples should be selected randomly from the population of interest. This means that each individual in the population has an equal chance of being included in any given sample. Random sampling ensures that the sample is representative of the entire population.\n",
    "\n",
    "# 2.>  Independence: The observations within each sample should be independent of each other. This implies that the value of one observation does not affect the value of another observation in the same sample. Additionally, the samples themselves should be drawn independently from the population.\n",
    "\n",
    "# 3.> Identically Distributed: The random variables in the population from which the samples are drawn should have the same probability distribution. In other words, they should have the same mean (μ) and standard deviation (σ). This ensures that the sample means are consistently estimating the same population mean.\n",
    "\n",
    "# 4.> Sample Size: The Central Limit Theorem holds for large sample sizes. As a general rule of thumb, a sample size of 30 or more is considered large enough for the CLT to apply. However, in practice, even smaller sample sizes can often produce approximately normal distributions.\n",
    "\n",
    "# It is important to note that violating these assumptions can lead to the breakdown of the Central Limit Theorem and may result in misleading conclusions. For example, if the samples are not selected randomly, they may not be representative of the population. If the observations are not independent, the variability of the sample means may be biased. If the population distribution is not identical or the sample size is too small, the convergence to a normal distribution may not occur.\n",
    "\n",
    "# Despite these assumptions, the Central Limit Theorem is a powerful tool in statistical analysis and inference, allowing researchers to draw meaningful conclusions about populations based on sample data, even when the underlying distribution is unknown or non-normally distributed.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6782db75-810d-41c1-99aa-9d8209f1b821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
